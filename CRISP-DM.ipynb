{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The lifecycle outlines the major stages that projects typically execute. There are six stages to build a model\n",
    "\n",
    "\n",
    "Business Understanding \n",
    "\n",
    "Data Understanding\n",
    "\n",
    "Data Preparation\n",
    "\n",
    "Evaluation\n",
    "\n",
    "Modeling \n",
    "\n",
    "Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I am Going to import nessesary module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "%matplotlib inline\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cale_data=pd.read_csv(\"calendar.csv\")\n",
    "list_data=pd.read_csv(\"listings.csv\")\n",
    "revi_data=pd.read_csv(\"reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cale_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the head of calendar datasets\n",
    "cale_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find missing values in calendar dataset\n",
    "cale_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find % missing values in each columns\n",
    "cale_data.isnull().sum()/cale_data.isnull().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cale_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cale_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.My First Question is that how the price varies with the Date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only available days are stored in data, it seems to be stored not available days.\n",
    "\n",
    "If the available values are f, the price values seems to be NaN.\n",
    "\n",
    "The price values are stored as object, not integer. \n",
    "This is caused the value stored like $xx.xx, and it is necessary to transform this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Start to cleaning the calender datasets\n",
    "def transformation(df):\n",
    "    '''input:That has to be clean\n",
    "    i am writing this function to convert the date object to data and converts the price \n",
    "    columns object to flot vlaues\n",
    "    output: will be a clean dataframe'''\n",
    "    df.dropna(inplace=True)\n",
    "    df['date']=pd.to_datetime(df['date'])\n",
    "    \n",
    "    df['price']=df['price'].str.replace('$','')\n",
    "    df['price']=df['price'].str.replace(\",\",'')\n",
    "    df['price']=df['price'].astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation(cale_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's aggrigate price based on the date for analys this question\n",
    "cale_data_df=cale_data.groupby(\"date\")['price'].mean().reset_index()\n",
    "cale_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see how the price very over a period of time by plotting a line chart\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(cale_data_df['date'],cale_data_df['price'])\n",
    "plt.title(\"Average price of a listing by date\")\n",
    "plt.xlabel(\"Average\")\n",
    "plt.ylabel(\"Average price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the same let's try to take  out the weekdays name from the data and the plot the price chart the weekdays\n",
    "cale_data_df[\"day of the week\"]=cale_data_df[\"date\"].dt.day_name()\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.boxplot(x='day of the week', y='price',data=cale_data_df ,palette=\"Blues\", width=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can understand that From the above visualisation,we figured out that the prices of \n",
    "the listings were maximum from July to September and were minimum from January to March"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.How long is the period available for lending by rooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there rooms which is available all for years? \n",
    "\n",
    "or almost rooms are available on spot like one day or one week?\n",
    "\n",
    "Here, I want to know the trend in the outline of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we are going to analys listing dataset\n",
    "#fistly look at the head of the data\n",
    "list_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now find the missing values in the listing dataset\n",
    "list_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if I want to see the % missing values in each column, i have use below line\n",
    "list_data.isnull().sum()/list_data.isnull().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Shape of listing data or no of rows and column in Listing \n",
    "list_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the number of id and row in the table\n",
    "print(\"Num of listings: \", list_data.id.count())\n",
    "print(\"Num of rows: \", list_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above statement we can see that every row represents uniqe listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now describe the data with 8 most important operations\n",
    "list_data['review_scores_rating'].describe().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see the histogram of review_scores_rating\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.hist(list_data['review_scores_rating'].dropna().values,bins=80,color='b')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very right skewed distribution.\n",
    "The 75% or more values are 90 points. And the most common thing is 100 points.\n",
    "I can say the low score listings are minolity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are going to estemate outliers becouse values are very large\n",
    "list_data = list_data[list_data['maximum_nights'] <= 1500]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.hist(list_data.maximum_nights, bins=100, color='b')\n",
    "plt.xlabel('maximum nights')\n",
    "plt.ylabel('listings count')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was shown when investigating listing data. There were two groups in listings. It's a listing available at spots with a maximum nights less than a week and a listing of a sense of renting available for up to three years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets plot the histogram of listings\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.hist(list_data.price, bins=100, color='r')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is long tail distribution.\n",
    "Almost values are from 0 to 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revi_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revi_data.isnull().sum()/revi_data.isnull().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revi_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data.head()\n",
    "list_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"sample 1: \", revi_data.comments.values[0], \"\\n\")\n",
    "print(\"sample 2: \", revi_data.comments.values[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date column's data type to date from object\n",
    "review_df= revi_data.copy(deep=True)\n",
    "review_df.date = pd.to_datetime(review_df.date)\n",
    "\n",
    "review_df = review_df.groupby('date')['id'].count().reset_index()\n",
    "\n",
    "# plot avg listings prices over time.\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(review_df.date, review_df.id, color='b', linewidth=0.9)\n",
    "plt.title(\"Number of reviews by date\")\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('number of reviews')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is little noisy, but we can see an increase in the number of Airbnb users. (and the date range is wide than calendar data)\n",
    "And I realize it seems to have a peak at about the same time of each year.\n",
    "So, let's use moving averages to smooth the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rolling mean column\n",
    "review_df[\"rolling_mean_30\"] = review_df.id.rolling(window=30).mean()\n",
    "\n",
    "# plot avg listings prices over time.\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(review_df.date, review_df.rolling_mean_30, color='b', linewidth=2.0)\n",
    "plt.title(\"Number of reviews by date\")\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('number of reviews')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried thirty days (about 1 month) window.\n",
    "The graph became smooth and the trend became clear, and my belief that the peaks were in the same place became stronger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df[\"year\"] = review_df.date.dt.year\n",
    "years = review_df.year.unique()\n",
    "\n",
    "for year in years:\n",
    "    if year >= 2010 and year < 2016:\n",
    "        year_df = review_df[review_df.year == year]\n",
    "        max_value = year_df.rolling_mean_30.max()\n",
    "        max_date = year_df[year_df.rolling_mean_30 == max_value].date.dt.date.values[0]\n",
    "        print(year, max_date, np.round(max_value, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For a further discussion, plot a scatter plot of maximum nights and minimum nights.\n",
    "list_data[\"min_max_night_diff\"] = list_data.maximum_nights - list_data.minimum_nights\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(list_data.maximum_nights, list_data.minimum_nights, color='b', marker='o', linewidth=0, alpha=0.25)\n",
    "plt.xlabel('maximum nights')\n",
    "plt.ylabel('minimum nights')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, it can be seen that the minimum nights is almost constant regardless of the maximum nights. In other words, it can be seen that listings with a long maximum nights are not rented exclusively for rental, but are widely handled from spot use to long-term stay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3.Is there a busy season?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df2 = review_df[review_df.year == 2015]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(review_df2.date, review_df2.rolling_mean_30, color='b', linewidth=2.0)\n",
    "plt.title(\"Number of reviews by date\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4.Are there any trends of popular rooms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define the target variable 'Popular of listings'. I thought this could be defined as follows.\n",
    "\n",
    "[Actual number of times rent] / ([Available days from 2016 to 2017] * (2017 - [Year of the listings open]))\n",
    "\n",
    "Since the number of times the listings are actually rent is considered to be proportional to the number of days the property is available, it needs to be scaled, and Furthermore, since the number is considered to increase the earlier the listings is released to the public, we need to scale there as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_df = list_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null count\n",
    "df_length = prepare_df.shape[0]\n",
    "\n",
    "for col in prepare_df.columns:\n",
    "    null_count = prepare_df[col].isnull().sum()\n",
    "    if null_count == 0:\n",
    "        continue\n",
    "        \n",
    "    null_ratio = np.round(null_count/df_length * 100, 2)\n",
    "    print(\"{} has {} null values ({}%)\".format(col, null_count, null_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it seems that there are columns 90% or more of the values are null, but most of the columns have between 0-30% of null_ratio. Therefore, here, I decided to excluded from analysis the columns with 30% or more null ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect need drop columns\n",
    "drop_cols = [col for col in prepare_df.columns if prepare_df[col].isnull().sum()/df_length >= 0.3]\n",
    "\n",
    "# drop null\n",
    "prepare_df.drop(drop_cols, axis=1, inplace=True)\n",
    "prepare_df.dropna(subset=['host_since'], inplace=True)\n",
    "\n",
    "# check after\n",
    "for col in prepare_df.columns:\n",
    "    null_count = prepare_df[col].isnull().sum()\n",
    "    if null_count == 0:\n",
    "        continue\n",
    "        \n",
    "    null_ratio = np.round(null_count/df_length * 100, 2)\n",
    "    print(\"{} has {} null values ({}%)\".format(col, null_count, null_ratio))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good.\n",
    "Next, we will delete the column that seems not to be a feature of ease of borrowing. Since we do not include natural language processing this time, we will also delete comment-based columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['listing_url', 'scrape_id', 'last_scraped', 'name', 'summary', 'space', 'description', 'neighborhood_overview',\n",
    "                'transit', 'medium_url', 'picture_url', 'xl_picture_url', 'host_id', 'host_url', 'host_name', 'host_about', 'host_thumbnail_url',\n",
    "                'host_picture_url', 'street', 'city', 'state', 'zipcode', 'market', 'smart_location', 'country_code', 'country', 'latitude', 'longitude',\n",
    "                'calendar_updated', 'calendar_last_scraped', 'first_review', 'last_review', 'amenities', 'host_verifications']\n",
    "\n",
    "prepare_df.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, delete columns that have only a single value, because the feature value does not make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = []\n",
    "for col in prepare_df.columns:\n",
    "    if prepare_df[col].nunique() == 1:\n",
    "        drop_cols.append(col)\n",
    "        \n",
    "prepare_df.drop(drop_cols, axis=1, inplace=True)\n",
    "prepare_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Now I almost finished remove columns that is not used for anallysis. So next, I create target valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cale_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# available days count each listings\n",
    "listing_avalilable =cale_data.groupby('listing_id')['price'].count().reset_index()\n",
    "listing_avalilable.columns = [\"id\", \"available_count\"]\n",
    "\n",
    "# merge\n",
    "prepare_df = prepare_df.merge(listing_avalilable, how='left', on='id')\n",
    "\n",
    "# create target column\n",
    "prepare_df['host_since_year'] = pd.to_datetime(prepare_df['host_since']).dt.year\n",
    "prepare_df[\"easily_accomodated\"] = prepare_df.accommodates / (prepare_df.available_count+1) / (2017 - prepare_df.host_since_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before: {} columns\".format(prepare_df.shape[1]))\n",
    "\n",
    "drop_cols = ['host_since', 'accommodates', 'availability_30', 'availability_60', 'availability_90', 'availability_365',\n",
    "                'number_of_reviews', 'review_scores_rating', 'available_count', 'reviews_per_month', 'host_since_year', 'review_scores_value']\n",
    "\n",
    "prepare_df.drop(drop_cols, axis=1, inplace=True)\n",
    "print(\"After: {} columns\".format(prepare_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert true or false value to 1 or 0\n",
    "dummy_cols = ['host_is_superhost', 'require_guest_phone_verification', 'require_guest_profile_picture', 'instant_bookable', \n",
    "              'host_has_profile_pic', 'host_identity_verified', 'is_location_exact']\n",
    "\n",
    "for col in dummy_cols:\n",
    "    prepare_df[col] = prepare_df[col].map(lambda x: 1 if x == 't' else 0)\n",
    "\n",
    "# create dummy valuables\n",
    "dummy_cols = ['host_location', 'host_neighbourhood', 'neighbourhood', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed',\n",
    "             'property_type', 'room_type', 'bed_type', 'cancellation_policy', 'host_response_time']\n",
    "\n",
    "prepare_df = pd.get_dummies(prepare_df, columns=dummy_cols, dummy_na=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_length = prepare_df.shape[0]\n",
    "\n",
    "for col in prepare_df.columns:\n",
    "    null_count = prepare_df[col].isnull().sum()\n",
    "    if null_count == 0:\n",
    "        continue\n",
    "        \n",
    "    null_ratio = np.round(null_count/df_length * 100, 2)\n",
    "    print(\"{} has {} null values ({}%)\".format(col, null_count, null_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, handle any remaining null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_df[\"is_thumbnail_setted\"] = 1 - prepare_df.thumbnail_url.isnull()\n",
    "prepare_df.drop('thumbnail_url', axis=1, inplace=True)\n",
    "prepare_df.host_response_rate = prepare_df.host_response_rate.fillna('0%').map(lambda x: float(x[:-1]))\n",
    "prepare_df.host_acceptance_rate = prepare_df.host_acceptance_rate.fillna('0%').map(lambda x: float(x[:-1]))\n",
    "prepare_df.bathrooms.fillna(0, inplace=True)\n",
    "prepare_df.bedrooms.fillna(0, inplace=True)\n",
    "prepare_df.beds.fillna(0, inplace=True)\n",
    "prepare_df.cleaning_fee.fillna('$0', inplace=True)\n",
    "prepare_df.review_scores_accuracy.fillna(0, inplace=True)\n",
    "prepare_df.review_scores_cleanliness.fillna(0, inplace=True)\n",
    "prepare_df.review_scores_checkin.fillna(0, inplace=True)\n",
    "prepare_df.review_scores_communication.fillna(0, inplace=True)\n",
    "prepare_df.review_scores_location.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since the value to be acted as a numeric value is recognized as a string, so let's convert it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in prepare_df.columns:\n",
    "    if prepare_df[col].dtypes == 'object':\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_df.price = prepare_df.price.map(lambda x: float(x[1:].replace(',', '')))\n",
    "prepare_df.cleaning_fee = prepare_df.cleaning_fee.map(lambda x: float(x[1:].replace(',', '')))\n",
    "prepare_df.extra_people = prepare_df.extra_people.map(lambda x: float(x[1:].replace(',', '')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prices of the listings are maximum from July to September and are minimum from January to March which is understood as July to September is the best time to visit Seattle because of the nice weather and therefore the prices are highest during that time because of the high demand.\n",
    "The average rating score of Arbor Heights is maximum followed by Riverview and Southeast Magnolia.The ratings of a listing is influenced by whether a host is a superhost or not,the host’s response rate,the size of the listings,the number of amenities provided in the listing and also on its price.\n",
    "Price of a listing depends on its neighborhood,size(number of bedrooms,number of bathrooms),type of the property i.e. if it is a bungalow,the price is higher than the other property and on whether the host is a superhost or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
